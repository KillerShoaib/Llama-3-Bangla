{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "* **Agenda: Finetune Llama-3 8b in Kaggle/Colab without hitting the time limit**\n",
        "* **Step-1: Install Unslot and other dependencies**\n",
        "* **Step-2: Login to Wandb and setup env variable**\n",
        "* **Step-3: Load the unsloth/llama-3-8b-bnb-4bit model**\n",
        "* **Step-4: Add LoRA Adapters (so we need to finetune 1-10% of the params)**\n",
        "* **Step-5: Load the Alpaca Clean Dataset & Structure it according to prompt**\n",
        "* **Step-6: Finetune the Model for 1st Iteration**\n",
        "* **Step-7. Finetune the Model for <code>N<sup>th</sup></code> Iteration**\n",
        "  * **Step-7.1: repeat `step 1-5` and skip `step-6` (skipping..)**\n",
        "  * **Step-7.2: Downloading Checkpoint Artifact from wandb**\n",
        "  * **Step-7.3: Structure the checkpoint so that training can be resumed**\n",
        "  * **Step-7.4: Start finetuning for n<sup>th</sup> iteartions using `resume_from_checkpoint =True`**\n",
        "* **Step-8: Saving the model Locally and Huggingface Hub**\n",
        "  * **Save only LoRA adapters (not the entire model)**\n",
        "  * **Save 16bit or 4bit Quantize Model**\n",
        "* **Step-9: Loading & Infercing the model**\n",
        "  * **Loading & Infercing with Unsloth `FastLanguageModel`**\n",
        "  * **Loading & Infercing with Huggingface `AutoPeftModelForCausalLM` (only for LoRA Adapter model)**\n",
        "  * **Loading & Infercing using with Huggingface `AutoModelForCausalLM` (for 4bit,16bit)**\n",
        "  ---\n",
        "\n"
      ],
      "metadata": {
        "id": "AktUUAmJ5pWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agenda: Finetune Llama-3 8b in Kaggle/Colab without hitting the time limit\n",
        "\n",
        "- The purpose of this script to show you how you can use **incremantal learning** inside **Kaggle** or **Colab** notebook. So, that you don't have to finetune the model on a single go.\n",
        "- Rather, you can finetune the model in such a way that it don't hit the time limit of kaggle (12hrs) / colab and finetune the model using `max_steps` instead of `num_train_epochs` and resume the training from previous checkpoint using `resume_checkpoint=True`.\n",
        "- All the checkpoints will be save in **weights & biases** so that we can download it for next run."
      ],
      "metadata": {
        "id": "g0mIDnPCPp-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-1: Install Unslot and other dependencies"
      ],
      "metadata": {
        "id": "zDIGbLMUf-Sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYEOOcJbfzN5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n",
        "!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-2: Login to Wandb and setup env variable\n",
        "\n",
        "**if wandb is not installed then use `!pip install wandb` to install it.**"
      ],
      "metadata": {
        "id": "KtlG_iyLhOd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"]=\"PROJECT_NAME\" # for project name, give an appropriate name\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" # for save the checkpoints\n",
        "wandb.login(key= \"YOUR_WANDB_API_KEY\") # replace it with your api key"
      ],
      "metadata": {
        "id": "VTsWr-XZhUXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-3: Load the unsloth/llama-3-8b-bnb-4bit model"
      ],
      "metadata": {
        "id": "by1FK_SXgMpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "id": "1TRTFlTugKlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-4: Add LoRA Adapters (so we need to finetune 1-10% of the params)"
      ],
      "metadata": {
        "id": "n2ms-EUpgbpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "LoBOKexAgbZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-5: Load the Alpaca Clean Dataset & Structure it according to prompt\n",
        "**Don't forget to add `EOS` token. Otherwise finetuned model won't learn to predict the eos token and text generation won't stop**"
      ],
      "metadata": {
        "id": "k_wJhsk0gs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction in bangla that describes a task, paired with an input also in bangla that provides further context. Write a response in bangla that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"iamshnoo/alpaca-cleaned-bengali\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "CPXMUTzpgrs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-6: Finetune the Model for 1st Iteration"
      ],
      "metadata": {
        "id": "QKNS5VxfhEdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 1000,                #### Setting max_steps for 1000. (1-1000)\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        save_steps=500,                 ### Checkpoint will be save after every 500 steps\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        report_to=\"wandb\",  # reporting logs and checkpoint to wandb\n",
        "        run_name=\"1stIteration_TillYourStepNumber\", ### wandb run name, give appropriate run name according to your choice ###\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",   # Saving the checkpoints to outputs folder\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "XvLzVw37hBR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Here `max_steps=1000`. means the finetuning will happen from 0-1000 steps.**\n",
        "- **And, `report_to=\"wandb\"` means train loss will log in wandb and artifacts also**\n",
        "- **`save_steps=500` means in every 500 steps a checkpoint of the `model` with that `trainer state` will be save.**\n",
        "- **Finally, `output_dir=\"outputs\"` all the model checkpoint will be save locally in that folder**"
      ],
      "metadata": {
        "id": "r7UExCBOiSPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Start training\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "LG8sq6nHjc3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish() # Finish Wandb Run"
      ],
      "metadata": {
        "id": "YDJX6uY_jujE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-7. Finetune the Model for <code>N<sup>th</sup></code> Iteration\n",
        "\n",
        "- **Step-7.1: repeat `step 1-5` and skip `step-6`**\n",
        "- **Step-7.2: Download the last `checkpoint artifact` from wandb.**\n",
        "- **Step-7.3: Structure the checkpoint so that training can be resumed**\n",
        "- **Step-7.4: Start finetuning for n<sup>th</sup> iteartions using `resume_from_checkpoint =True`**\n"
      ],
      "metadata": {
        "id": "zBm4wVWFjoIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-7.1: repeat `step 1-5` and skip `step-6` (skipping..)"
      ],
      "metadata": {
        "id": "AiSlZwh3mgZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-7.2: Downloading Checkpoint Artifact from wandb\n",
        "\n",
        "- **Go to wandb website**\n",
        "- **Select the project (your finetune llama project)**\n",
        "- **Go to Workspace**\n",
        "- **Click on artifact tab (on the bottom left)**\n",
        "- **Check for `checkpoint-yourPreviousWandbRunName` and click it.**\n",
        "- **You'll see many artifact starting like this `v0`,`v1`,...`vn`. Now select the `vn` latest checkpoint**\n",
        "- **You'll see is a top tab showing `Version`, `Metadata`, `Usage`... from there select `Usage` tab.**\n",
        "- **Copy the code & Paste it here**\n",
        "- **Run the cell**"
      ],
      "metadata": {
        "id": "V9mKuPgempxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The code will look like this\n",
        "run = wandb.init()\n",
        "artifact = run.use_artifact('YOUR_ARTIFACT_URL', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "run.finish()  ### Don't forget to add this line"
      ],
      "metadata": {
        "id": "OvAAS7x6l3ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remeber to finish the run `run.finish()`.**"
      ],
      "metadata": {
        "id": "TfBUiCjUo9AK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-7.3: Structure the checkpoint so that training can be resumed"
      ],
      "metadata": {
        "id": "PVYZqcREpELT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **It is a helper function, which is just moving all the files from the `artifact` folder to the `outputs` folder**\n",
        "- **Only change `dst_dir` value at the bottom.**\n",
        "- **For `Colab` change the value to `/content/outputs/checkpoint-PreviousCheckPointNumber`**\n",
        "- **For `Kaggle` change the value to `/kaggle/working/outputs/checkpoint-PreviousCheckPointNumber`**\n",
        "- **`PreviousCheckPointNumber` means the last checkpoint value that is saved in the wandb run and you downloaded in the previous cell.**"
      ],
      "metadata": {
        "id": "ouYZ4DfDpjhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fileStructure(src_dir,dst_dir):\n",
        "    import shutil\n",
        "    import os\n",
        "    # Create the destination directory if it doesn't exist\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "    # Get a list of all file names in the source directory\n",
        "    file_names = os.listdir(src_dir)\n",
        "\n",
        "    if len(os.listdir(dst_dir))==0:\n",
        "        # Move each file to the destination directory\n",
        "        for file_name in file_names:\n",
        "            shutil.move(os.path.join(src_dir, file_name), dst_dir)\n",
        "    else:\n",
        "        print(\"Files already been moved\")\n",
        "\n",
        "\n",
        "# Define source and destination directories\n",
        "src_dir = artifact_dir\n",
        "dst_dir = \"/kaggle/working/outputs/checkpoint-PreviousCheckPointNumber\"   ########## Change This Path Everytime with the Proper Checkpoint Number ###############\n",
        "\n",
        "\n",
        "# Run the function\n",
        "fileStructure(src_dir,dst_dir)"
      ],
      "metadata": {
        "id": "R2i8P2UZo8kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.rmdir(src_dir)\n",
        "except:\n",
        "    # The directory is not empty, use shutil.rmtree() instead\n",
        "    import shutil\n",
        "    shutil.rmtree(src_dir)"
      ],
      "metadata": {
        "id": "jCXbfRDKrs6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Above cell is optional. It is removing artifact dir. We don't need it anymore**"
      ],
      "metadata": {
        "id": "uT9NEBm_r7Ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-7.4: Start finetuning for n<sup>th</sup> iteartions using `resume_from_checkpoint =True`"
      ],
      "metadata": {
        "id": "sHJjDdNKtuz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- change `max_steps` to **(previous max steps + new nth max step)**. i.e: previouse max steps = 1000 and new nth max steps = 1000, then `max_steps=2000`.\n",
        "- change `run_name` to your appropriate wandb run name."
      ],
      "metadata": {
        "id": "oEySDgR6t8Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = ,   ### max_steps = prev_max_steps+new_nth_max_steps ###\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        save_steps=500,   ### Saving the checkpoints in every 500 steps ###\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=\"NthIteration_TillYourStepNumber\",  ### Give appropriate run name according to your choice ###\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\", ### Saving the checkpoints to outputs folder ###\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "I5OSYTCWr6hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Change `max_steps` and `run_name`**"
      ],
      "metadata": {
        "id": "UQxkwUhivwT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start training the trainer using `resume_from_checkpoint =True`**"
      ],
      "metadata": {
        "id": "wR0hlhCjwjY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train(resume_from_checkpoint = True)"
      ],
      "metadata": {
        "id": "XdFZrKJqwe3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finish wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "nWXmUtK2wx2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Kaggle/Colab Training\n",
        "\n",
        "- Now run from **step 1-6** for **first** time.\n",
        "- run from **step 1-5** and **step 7** after the first time.\n",
        "- Always run the code in **T4** gpu.\n",
        "- get total `max_steps` requires for 1 epoch. (**Try to run the trainer with `num_train_epoch=1` and start training, it'll show total max steps require to finish 1 epoch. remember that number**)\n",
        "- Set `max_steps` value in such way that all the steps can be completed before the time limit. ( i.e: **12hrs for kaggle** )\n",
        "- Use **Kaggle** save version to commit the code (so that it can run automatically)\n",
        "  - Comment out every code that won't be running in that version. i.e: **1st iteration** step **7-9** will be comment out and **after 1st iteration** step **6** & **8-9** will be comment out.\n",
        "  - select a version name ( give an appropriate name ).\n",
        "  - in `version type` select `Save & Run All (commit)`.\n",
        "  - Click on Advance setting inside `Run with GPU for this session`\n",
        "  - Click `Save` in the bottom to commit the code.\n",
        "  - Make sure before commiting the notebook the notebook was running or currently **running in the T4 GPU.**\n",
        "\n",
        "- Now repeat this process till completing total `max_steps` require to complete 1 epochs or 2 epoch.\n",
        "- At the last iteration where you'll complete the `max_steps` require for 1/2 epochs. Immediately follow **step 8**. for Kaggle `save version` try to **add the Step 8 code** for saving the model in the end ( **Only for last iteration** ).\n",
        "\n",
        "---\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "uLSGWki-SFei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-8: Saving the model Locally and Huggingface Hub\n",
        "\n",
        "save the model and tokenizer after the last iteration.**‚ùó Remember to save the model immediately after last iteration. checkpoint can't be loaded as model**"
      ],
      "metadata": {
        "id": "ZY-fYgv8xB2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save only LoRA adapters (not the entire model)"
      ],
      "metadata": {
        "id": "tvcTgDdxx4E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save it locally\n",
        "if False:\n",
        "  model.save_pretrained(\"lora_model\")\n",
        "  tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "ne2PvuTBxBYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save it to Huggingface hub\n",
        "if False:\n",
        "  model.push_to_hub(\"your_name/lora_model\", token = \"your huggingface token with write permission\")\n",
        "  tokenizer.push_to_hub(\"your_name/lora_model\", token = \"your huggingface token with write permission\")"
      ],
      "metadata": {
        "id": "xocv9BhyyVnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save 16bit or 4bit Quantize Model"
      ],
      "metadata": {
        "id": "wFCNZyMXysJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to 4bit\n",
        "\n",
        "#Locally\n",
        "if False: model.save_pretrained_merged(\"4bit_model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "\n",
        "#HF hub\n",
        "if False: model.push_to_hub_merged(\"your_hf_username/4bit_model\", tokenizer, save_method = \"merged_4bit_forced\", token = \"your huggingface token with write permission\")"
      ],
      "metadata": {
        "id": "ibjpmtd_y23p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to 16bit\n",
        "\n",
        "# Locally\n",
        "if False: model.save_pretrained_merged(\"16bit_model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "\n",
        "#HF Hun\n",
        "if False: model.push_to_hub_merged(\"your_hf_username/4bit_model\", tokenizer, save_method = \"merged_16bit\", token = \"your huggingface token with write permission\")"
      ],
      "metadata": {
        "id": "eOMT51IMzK_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-9: Loading & Infercing the model"
      ],
      "metadata": {
        "id": "im_MdcUTz5Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading & Infercing with Unsloth `FastLanguageModel`"
      ],
      "metadata": {
        "id": "4Xem_3WS0Ela"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    max_seq_length=2048\n",
        "    dtype = None\n",
        "    load_in_4bit = True\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"\", # YOUR MODEL YOU USED FOR TRAINING either hf hub name or local folder name.\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "kHOlXp7fz-ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infercing**"
      ],
      "metadata": {
        "id": "6XVeGVpH0XOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction in bangla that describes a task, paired with an input also in bangla that provides further context. Write a response in bangla that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "VYsP42eh0ZSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infercing using `TextStreamer`**"
      ],
      "metadata": {
        "id": "xB1WxvLb0dDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "alpaca_prompt = \"\"\"Below is an instruction in bangla that describes a task, paired with an input also in bangla that provides further context. Write a response in bangla that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2048,eos_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "ertNd2xs0fye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading & Infercing with Huggingface `AutoPeftModelForCausalLM` (only for LoRA Adapter model)"
      ],
      "metadata": {
        "id": "kz1kIw4B0zqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    load_in_4bit = True\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"\", # YOUR MODEL YOU USED FOR TRAINING either hf hub name or local folder name.\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"\") # YOUR MODEL YOU USED FOR TRAINING either hf hub name or local folder name."
      ],
      "metadata": {
        "id": "xJkgm6IW07pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infercing**"
      ],
      "metadata": {
        "id": "Sq2tS5f61MzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction in bangla that describes a task, paired with an input also in bangla that provides further context. Write a response in bangla that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "DwNxH-dm1KeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading & Infercing with Huggingface `AutoModelForCausalLM` (for 4bit,16bit)"
      ],
      "metadata": {
        "id": "Vqn5hj9V1VOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "  model_name = \"\"  # YOUR MODEL YOU USED FOR TRAINING either hf hub name or local folder name.\n",
        "  tokenizer_name = model_name\n",
        "\n",
        "  # Load tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "  # Load model\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "J6PDbIbs1dyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infercing**"
      ],
      "metadata": {
        "id": "XWkMvUdS1pGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text prompt to start generation\n",
        "alpaca_prompt = \"\"\"Below is an instruction in bangla that describes a task, paired with an input also in bangla that provides further context. Write a response in bangla that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Encode the prompt text\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# output\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "2hNVui6o1mxI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}